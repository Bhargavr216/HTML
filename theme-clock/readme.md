**UNIT I | Q1: WHAT IS MACHINE LEARNING?**
Machine Learning enables computers to learn patterns from data without explicit programming. Systems improve performance through experience rather than hardcoded rules.

**ðŸ”¹ Definition (4 words)**
Learning from data automatically

**ðŸ”¹ Core Concept**
Data + Algorithm â†’ Model â†’ Predictions

**ðŸ”¹ Importance (4 words)**
Automates complex decision making

**ðŸ”¹ Real Examples**
* Healthcare: Disease prediction
* Finance: Fraud detection
* E-commerce: Recommendations

---

**UNIT I | Q2: TYPES OF ML LEARNING**
Four learning types based on data availability and feedback mechanism.



**ðŸ”¹ Supervised Learning (4 words)**
Labeled data input-output pairs
â†’ Classification & Regression
â†’ Algorithms: Decision Trees, SVM

**ðŸ”¹ Unsupervised Learning (4 words)**
Discovers hidden patterns unlabeled
â†’ Clustering & Dimensionality Reduction
â†’ Algorithms: K-Means, PCA

**ðŸ”¹ Semi-Supervised (4 words)**
Small labeled + large unlabeled
â†’ Uses both data types

**ðŸ”¹ Reinforcement Learning (4 words)**
Agent learns via rewards
â†’ Trial-and-error decision making
â†’ Algorithms: Q-Learning

---

**UNIT I | Q4: SUPERVISED LEARNING**
Learning mapping function from labeled input-output examples to predict unseen data.

**ðŸ”¹ How It Works (5 words)**
Observe patterns â†’ Generalize â†’ Predict
Training Data â†’ Model â†’ Predictions

**ðŸ”¹ Types**
* Classification: Discrete output (Spam/Not Spam)
* Regression: Continuous output (House price)

**ðŸ”¹ Algorithms**
Logistic Regression, Decision Trees, SVM, KNN

---

**UNIT I | Q5: REGRESSION VS CLASSIFICATION**

| Aspect | Regression | Classification |
| :--- | :--- | :--- |
| **Output** | Continuous | Categorical |
| **Example** | â‚¹50,000 | Spam/Not Spam |
| **Algorithm** | Linear Regression | Logistic Regression |
| **Evaluation** | MSE, RMSE | Accuracy, F1-score |

---

**UNIT I | Q7: LINEAR REGRESSION**
Predicts continuous output using linear relationship between input features and target variable.

**ðŸ”¹ Model Equation**
$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$

**ðŸ”¹ Assumptions (4 words)**
Linearity, Independence, Homoscedasticity, Normality

---

**UNIT I | Q9: LOGISTIC REGRESSION**
Classification algorithm predicting probability of binary outcome using sigmoid function.

**ðŸ”¹ Sigmoid Function**
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**ðŸ”¹ Decision Rule**
* Probability $\ge$ 0.5 â†’ Class 1
* Probability < 0.5 â†’ Class 0

---

**UNIT II | Q3: FEATURE SELECTION IMPORTANCE**
Selecting relevant features improves model accuracy, reduces overfitting, and lowers computational cost.

**ðŸ”¹ Why Important (5 words)**
Better accuracy, less complexity
* Removes noise & redundancy
* Handles curse of dimensionality
* Improves interpretability

---

**UNIT II | Q6: EVALUATION METRICS**
Metrics derived from confusion matrix to assess binary classifier performance.

**ðŸ”¹ Confusion Matrix**
| | Predicted Positive | Predicted Negative |
| :--- | :--- | :--- |
| **Actual Positive** | TP | FN |
| **Actual Negative** | FP | TN |

**ðŸ”¹ Key Formulas**
* Accuracy = $(TP+TN)/(TP+TN+FP+FN)$
* Precision = $TP/(TP+FP)$
* Recall = $TP/(TP+FN)$
* F1 = $2 \times \frac{Precision \times Recall}{Precision + Recall}$

---

**UNIT III | Q1: DECISION TREE REPRESENTATION**
Tree-structured model using if-else rules for classification/regression decisions.



**ðŸ”¹ Components (4 words)**
Nodes, branches, leaves structure
* Root: Top decision node
* Internal: Attribute tests
* Leaf: Final class label

---

**UNIT III | Q5: SUPPORT VECTOR MACHINES (SVM)**
Finds optimal hyperplane maximizing margin between different classes.



**ðŸ”¹ Equation**
$$w \cdot x + b = 0$$

**ðŸ”¹ Key Elements (4 words)**
Hyperplane, margin, support vectors

---

**UNIT IV | Q2: K-NEAREST NEIGHBOUR (KNN)**
Classifies based on majority class of k nearest training points.

**ðŸ”¹ Distance Formula (Euclidean)**
$$d = \sqrt{\sum (x_i - y_i)^2}$$

---

**UNIT IV | Q3: K-MEANS CLUSTERING**
Partitions data into K clusters by minimizing intra-cluster distance.



**ðŸ”¹ Objective Function**
$$J = \sum_{j=1}^{k} \sum_{i=1}^{n} \|x_i^{(j)} - \mu_j\|^2$$

---

**UNIT V | Q4: BACKPROPAGATION ALGORITHM**
Trains multi-layer networks by propagating error backward to update weights.

**ðŸ”¹ Weight Update**
$$w_{new} = w_{old} - \eta \cdot \frac{\partial E}{\partial w}$$

**ðŸ”¹ Steps**
1. Forward pass: Compute output
2. Calculate error (MSE)
3. Backward pass: Propagate error
4. Compute gradients
5. Update weights

---

**UNIT V | Q6: REINFORCEMENT LEARNING FRAMEWORK**
Agent learns optimal behavior through environment interaction and rewards.



**ðŸ”¹ Interaction Cycle**
State â†’ Action â†’ Reward â†’ New State

---

**UNIT V | Q9: Q-LEARNING ALGORITHM**
Model-free RL algorithm learning optimal action-value function.

**ðŸ”¹ Update Equation**
$$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max Q(s',a') - Q(s,a)]$$
